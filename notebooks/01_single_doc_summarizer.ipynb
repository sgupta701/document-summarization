{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ba3a642-b786-423b-a434-576fcaf8fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and OCR ready\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "from odf import text, teletype, opendocument\n",
    "import os, json\n",
    "\n",
    "# Path to your Tesseract installation\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "print(\"Libraries and OCR ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54b9b09f-19ab-4cea-8973-115688bf2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    text_data = \"\"\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        try:\n",
    "            # Step 1: Normal text extraction\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text_data += page_text + \"\\n\"\n",
    "\n",
    "            # Step 2: If no text found → OCR fallback\n",
    "            if not text_data.strip():\n",
    "                print(\"⚠️ No text found in PDF — using OCR fallback.\")\n",
    "                pdf_doc = fitz.open(file_path)\n",
    "                for page_number, page in enumerate(pdf_doc, start=1):\n",
    "                    pix = page.get_pixmap(dpi=300)\n",
    "                    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                    ocr_text = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "                    text_data += f\"\\n\\n--- Page {page_number} ---\\n{ocr_text}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ PDF read error:\", e)\n",
    "            # Fallback OCR\n",
    "            pdf_doc = fitz.open(file_path)\n",
    "            for page_number, page in enumerate(pdf_doc, start=1):\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                ocr_text = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "                text_data += f\"\\n\\n--- Page {page_number} ---\\n{ocr_text}\"\n",
    "\n",
    "    elif ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "        img = Image.open(file_path)\n",
    "        text_data = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        doc = docx.Document(file_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text_data += para.text + \"\\n\"\n",
    "\n",
    "    elif ext == \".odt\":\n",
    "        doc = opendocument.load(file_path)\n",
    "        allparas = doc.getElementsByType(text.P)\n",
    "        for p in allparas:\n",
    "            text_data += teletype.extractText(p) + \"\\n\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    return text_data.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29d18ec0-19b7-4a24-a89f-08fd82a4b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# define model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# load model and tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# create summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Summarization model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cde0009-229c-4f4d-a9b4-6c9792e226f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_chunk_tokens=900):\n",
    "    \"\"\"\n",
    "    Summarizes any length of text — short or long.\n",
    "    Automatically splits into chunks (if too long),\n",
    "    then combines and refines the summary.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text or not text.strip():\n",
    "        return \"⚠️ No text found to summarize.\"\n",
    "\n",
    "    # short documents \n",
    "    if len(text.split()) < 100:\n",
    "        result = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
    "        return result[0]['summary_text']\n",
    "\n",
    "    # Tokenize text into chunks\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    token_chunks = [inputs[i:i + max_chunk_tokens] for i in range(0, len(inputs), max_chunk_tokens)]\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in token_chunks:\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        try:\n",
    "            summary = summarizer(chunk_text, max_length=250, min_length=80, do_sample=False)\n",
    "            summaries.append(summary[0]['summary_text'])\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Skipped one chunk due to:\", e)\n",
    "            continue\n",
    "\n",
    "    combined_summary = \" \".join(summaries)\n",
    "\n",
    "    # Compress again if multiple chunks\n",
    "    if len(summaries) > 1:\n",
    "        final_summary = summarizer(\n",
    "            combined_summary,\n",
    "            max_length=300,\n",
    "            min_length=100,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        return final_summary\n",
    "    else:\n",
    "        return combined_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3eef2e8-cc2f-406d-8f11-0f2a4d6bb7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Define model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# create summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "print(\"Summarization model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dd5f557-93e4-425d-ac90-b2401acd7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_chunk_tokens=900):\n",
    "    \"\"\"\n",
    "    Summarizes any length of text — short or long.\n",
    "    - Automatically splits into chunks (if too long)\n",
    "    - Safely combines partial summaries\n",
    "    - Produces one clean final summary\n",
    "    \"\"\"\n",
    "    # short text \n",
    "    if len(text.split()) < 100:\n",
    "        result = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
    "        return result[0]['summary_text']\n",
    "\n",
    "    # Tokenize full text into IDs (no truncation)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "\n",
    "    # Split tokens safely\n",
    "    token_chunks = [inputs[i:i + max_chunk_tokens] for i in range(0, len(inputs), max_chunk_tokens)]\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in token_chunks:\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        try:\n",
    "            summary = summarizer(\n",
    "                chunk_text,\n",
    "                max_length=250,\n",
    "                min_length=80,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Skipped one chunk due to:\", e)\n",
    "            continue\n",
    "\n",
    "    # Combine all small summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "\n",
    "    # If multiple chunks, compress again\n",
    "    if len(summaries) > 1:\n",
    "        final_summary = summarizer(\n",
    "            combined_summary,\n",
    "            max_length=300,\n",
    "            min_length=100,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        return final_summary\n",
    "    else:\n",
    "        return combined_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0b8deb9-9a5e-4076-a89a-fbe64b4fa5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No text found in PDF — using OCR fallback.\n",
      "Extracted 1287 characters from the file.\n",
      "\n",
      "Summary:\n",
      "\n",
      "The Bareilly City Police will be organizing an awareness programme titled “Nave Apradhik Kanoon ke Prati Jagrukta Abhiyan’ atSRMS_ College of Engineering and Technology, Bareilly. The programme will take place on November 25, 2025 (Saturday) at 2:00 PM in the NewSeminar Hall. All B.Tech First Year students are required to attend the programme.\n"
     ]
    }
   ],
   "source": [
    "sample_path = r\"E:\\single-doc-summarizer\\data\\raw\\notice1.pdf\"\n",
    "\n",
    "extracted = extract_text_from_file(sample_path)\n",
    "print(f\"Extracted {len(extracted)} characters from the file.\")\n",
    "\n",
    "summary = generate_summary(extracted)\n",
    "\n",
    "print(\"\\nSummary:\\n\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "719242a6-2232-4926-9905-ef8c2c607e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Summary saved to: E:\\single-doc-summarizer\\data\\processed\\notice1_summary.txt\n"
     ]
    }
   ],
   "source": [
    "output_folder = r\"E:\\single-doc-summarizer\\data\\processed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_folder, \"notice1_summary.txt\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n Summary saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a3320-2076-416f-9608-48fc45af304a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Single Summarizer",
   "language": "python",
   "name": "single-summarizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
